{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ca5a00d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time, cv2, dlib, numpy as np\n",
    "from collections import deque, Counter\n",
    "from IPython.display import display, clear_output\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "\n",
    "import random, subprocess, threading\n",
    "\n",
    "\"\"\"\n",
    "Here we point to the dlib landmark model file.\n",
    "Before we do anything else, check that the landmark model file actually exists.\n",
    "If it doesn't, stop immediately and tell the user what’s missing.\n",
    "\"\"\"\n",
    "\n",
    "BASE_DIR = Path.cwd()\n",
    "PREDICTOR_PATH = BASE_DIR / \"models\" / \"shape_predictor_68_face_landmarks.dat\"\n",
    "\n",
    "assert PREDICTOR_PATH.exists(), (\n",
    "    \"Model file not found. Please download \"\n",
    "    \"shape_predictor_68_face_landmarks.dat and place it in the models/ folder.\"\n",
    ")\n",
    "\n",
    "\"\"\"\n",
    "Here we create:\n",
    "1) a face detector (finds faces in an image)\n",
    "2) a landmark predictor (returns 68 landmark points for a detected face)\n",
    "\"\"\"\n",
    "detector  = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(str(PREDICTOR_PATH))\n",
    "\n",
    "\"\"\"\n",
    "Here we store the landmark indices we care about.\n",
    "Dlib’s 68-point scheme is fixed, so these indexes map to specific facial regions.\n",
    "\"\"\"\n",
    "\n",
    "IDX = {\n",
    "    \"mouth_left\": 48, \"mouth_right\": 54,\n",
    "    \"mouth_in_top\": 62, \"mouth_in_bot\": 66,\n",
    "    \"eye_L_outer\": 36, \"eye_R_outer\": 45,\n",
    "    \"eye_L\": [36,37,38,39,40,41],\n",
    "    \"eye_R\": [42,43,44,45,46,47],\n",
    "    \"brow_L\": [17,18,19,20,21],\n",
    "    \"brow_R\": [22,23,24,25,26],\n",
    "}\n",
    "\n",
    "# --- helpers ---\n",
    "def shape_to_np(shape):\n",
    "    #Here we convert dlib’s landmark object into a simple array. That makes it easier to do vector math.\n",
    "    return np.array([(p.x, p.y) for p in shape.parts()], dtype=np.float32)\n",
    "\n",
    "def dist(a, b):\n",
    "    # Here we compute Euclidean distance between two 2D points.\n",
    "    return float(np.linalg.norm(a - b))\n",
    "\n",
    "def center(pts):\n",
    "    # Here we get the average position of a group of points (useful for “center of mouth”, for instance)\n",
    "    return np.mean(pts, axis=0)\n",
    "\n",
    "def eye_ear(p):\n",
    "    # Here we compute the Eye Aspect Ratio (EAR).\n",
    "    # EAR goes down when the eye closes and goes up when the eye opens wide.\n",
    "    # Formula uses two vertical distances divided by the horizontal distance.\n",
    "    A = dist(p[1], p[5]); B = dist(p[2], p[4]); C = dist(p[0], p[3])\n",
    "    return (A + B) / (2.0*C + 1e-6)\n",
    "\n",
    "# Here we define colors per emotion label\n",
    "COL = {\n",
    "    \"neutral\":   (255,255,255),\n",
    "    \"happy\":     (0,255,0),\n",
    "    \"surprised\": (0,255,255),\n",
    "    \"sad\":       (255,255,0),\n",
    "    \"angry\":     (0,0,255),\n",
    "}\n",
    "\n",
    "# Here we keep a small pool of possible lines for each label.\n",
    "# The app picks one randomly so it doesn’t repeat the same sentence every time.\n",
    "REACTIONS = {\n",
    "    \"happy\": [\n",
    "        \"That smile looks good on you.\",\n",
    "        \"You look happy right now.\",\n",
    "        \"Someone's in a good mood.\",\n",
    "        \"Love that energy.\"\n",
    "    ],\n",
    "    \"sad\": [\n",
    "        \"Oh, you seem sad.\",\n",
    "        \"You look a little down. You okay?\",\n",
    "        \"You look extra sad. Has anything happened?\",\n",
    "        \"That face says I need a hug.\"\n",
    "    ],\n",
    "    \"angry\": [\n",
    "        \"Whoa, that's an angry face.\",\n",
    "        \"You look annoyed. Should I back away?\",\n",
    "        \"Okay, okay, I get it. You're mad.\",\n",
    "        \"That glare is intense.\"\n",
    "    ],\n",
    "    \"surprised\": [\n",
    "        \"You look so surprised.\",\n",
    "        \"Big eyes. What happened?\",\n",
    "        \"That was a real shock face.\",\n",
    "        \"Whoa, did I scare you?\"\n",
    "    ],\n",
    "    \"neutral\": [\n",
    "        \"You're chill.\",\n",
    "        \"Calm mode.\",\n",
    "        \"You look focused.\",\n",
    "        \"Very composed right now.\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "\"\"\"\n",
    "Feature meanings:\n",
    "mw = mouth_width\n",
    "mo = mouth_open\n",
    "bg = brow_gap (avg brow-eye vertical distance)\n",
    "curve = mouth corners vs center (<0 smile, >0 frown)\n",
    "eye_open = EAR (higher = eyes wider)\n",
    "dbr = delta_brow: outer minus inner brow gap, positive means inner brows pulled down\n",
    "\"\"\"\n",
    "\n",
    "# Here we store the last N predicted labels, then take the most common label for smoothing.\n",
    "# Smoothing reduces flicker when your face is near a threshold.\n",
    "smooth_labels = deque(maxlen=12)\n",
    "\n",
    "# Here we track when we last spoke so we don’t spam audio constantly.\n",
    "# We set the minimum time between comments. The only exception: if the emotion label changes, we allow speaking sooner.\n",
    "last_spoken_time = 0.0\n",
    "last_spoken_label = None\n",
    "cooldown_seconds = 8.0\n",
    "\n",
    "# Here we keep all emotion thresholds in one place so they are easy to tune.\n",
    "# These values were tuned to my face and may need adjustment for other users.\n",
    "TH = {\n",
    "    \"surprise_mo\": 0.10, \"surprise_bg\": 0.24, \"surprise_ear\": 0.26,\n",
    "    \"angry_bg\": 0.24, \"angry_ear\": 0.32, \"angry_mo\": 0.10, \"angry_dbr\": 0.01,\n",
    "    \"happy_curve\": -0.05, \"happy_mw\": 0.60, \"happy_mo\": 0.28,\n",
    "    \"sad_curve\": 0.001, \"sad_mw\": 0.45, \"sad_mo\": 0.10,\n",
    "}\n",
    "\n",
    "\n",
    "def label_from_feats(mw, mo, bg, curve, eye_open, dbr):\n",
    "    # Here we map face features -> an emotion label using simple rules.\n",
    "\n",
    "    # surprised\n",
    "    # wide mouth + brows raised + eyes wide\n",
    "    if mo > TH[\"surprise_mo\"] and bg > TH[\"surprise_bg\"] and eye_open > TH[\"surprise_ear\"]:\n",
    "        return \"surprised\"\n",
    "    \n",
    "    # angry\n",
    "    # lowered brows + narrower eyes + closed mouth\n",
    "    # OR furrowed brow signal (dbr) with a closed mouth.\n",
    "    if (bg < TH[\"angry_bg\"] and eye_open < TH[\"angry_ear\"] and mo < TH[\"angry_mo\"]) or \\\n",
    "       (dbr > TH[\"angry_dbr\"] and mo < TH[\"angry_mo\"]):\n",
    "        return \"angry\"\n",
    "    \n",
    "    # happy\n",
    "    # smile curve (corners up) + wide mouth + not too open\n",
    "    if curve < TH[\"happy_curve\"] and mw > TH[\"happy_mw\"] and mo < TH[\"happy_mo\"]:\n",
    "        return \"happy\"\n",
    "    \n",
    "    # sad\n",
    "    # corners down + narrower mouth + closed mouth\n",
    "    if curve > TH[\"sad_curve\"] and mw < TH[\"sad_mw\"] and mo < TH[\"sad_mo\"]:\n",
    "        return \"sad\"\n",
    "    \n",
    "    # neutral\n",
    "    # Here we fall back to neutral if nothing matches.\n",
    "    return \"neutral\"\n",
    "\n",
    "\n",
    "def speak_async(sentence):\n",
    "    # Here we speak using macOS `say` without blocking the camera loop.\n",
    "    # We run `say` in a background thread so the main loop stays real-time.\n",
    "    def _do_say(s):\n",
    "        subprocess.call([\"say\", s])\n",
    "    threading.Thread(target=_do_say, args=(sentence,), daemon=True).start()\n",
    "\n",
    "def maybe_say(label):\n",
    "    global last_spoken_time, last_spoken_label\n",
    "\n",
    "    now = time.time()\n",
    "\n",
    "    # if you dislike neutral chatter, you can uncomment this:\n",
    "    # if label == \"neutral\":\n",
    "    #     return\n",
    "    \n",
    "    # Here we allow speech if:\n",
    "    # - enough time passed since last comment, OR\n",
    "    # - the emotion label changed (so we can react quickly to a new expression).\n",
    "    time_ok = (now - last_spoken_time) > cooldown_seconds\n",
    "\n",
    "    changed_ok = (label != last_spoken_label)\n",
    "    \n",
    "    # Here we pick a random sentence from the pool and speak it.\n",
    "    if (time_ok or changed_ok) and (label in REACTIONS):\n",
    "        sentence = random.choice(REACTIONS[label])\n",
    "        speak_async(sentence)\n",
    "        last_spoken_time = now\n",
    "        last_spoken_label = label\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "036c409e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "camera released\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m~/opt/anaconda3/envs/mari/lib/python3.9/site-packages/PIL/ImageFile.py:644\u001b[0m, in \u001b[0;36m_save\u001b[0;34m(im, fp, tile, bufsize)\u001b[0m\n\u001b[1;32m    643\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 644\u001b[0m     fh \u001b[38;5;241m=\u001b[39m \u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfileno\u001b[49m()\n\u001b[1;32m    645\u001b[0m     fp\u001b[38;5;241m.\u001b[39mflush()\n",
      "\u001b[0;31mAttributeError\u001b[0m: '_idat' object has no attribute 'fileno'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [9]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    118\u001b[0m rgb \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcvtColor(frame, cv2\u001b[38;5;241m.\u001b[39mCOLOR_BGR2RGB)\n\u001b[1;32m    119\u001b[0m clear_output(wait\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 120\u001b[0m \u001b[43mdisplay\u001b[49m\u001b[43m(\u001b[49m\u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfromarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrgb\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;66;03m# Tiny sleep so notebook doesn't get spammed\u001b[39;00m\n\u001b[1;32m    123\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.005\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/mari/lib/python3.9/site-packages/IPython/core/display_functions.py:298\u001b[0m, in \u001b[0;36mdisplay\u001b[0;34m(include, exclude, metadata, transient, display_id, raw, clear, *objs, **kwargs)\u001b[0m\n\u001b[1;32m    296\u001b[0m     publish_display_data(data\u001b[38;5;241m=\u001b[39mobj, metadata\u001b[38;5;241m=\u001b[39mmetadata, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    297\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 298\u001b[0m     format_dict, md_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minclude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minclude\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexclude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexclude\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    299\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m format_dict:\n\u001b[1;32m    300\u001b[0m         \u001b[38;5;66;03m# nothing to display (e.g. _ipython_display_ took over)\u001b[39;00m\n\u001b[1;32m    301\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/mari/lib/python3.9/site-packages/IPython/core/formatters.py:178\u001b[0m, in \u001b[0;36mDisplayFormatter.format\u001b[0;34m(self, obj, include, exclude)\u001b[0m\n\u001b[1;32m    176\u001b[0m md \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 178\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mformatter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;66;03m# FIXME: log the exception\u001b[39;00m\n\u001b[1;32m    181\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/mari/lib/python3.9/site-packages/decorator.py:232\u001b[0m, in \u001b[0;36mdecorate.<locals>.fun\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kwsyntax:\n\u001b[1;32m    231\u001b[0m     args, kw \u001b[38;5;241m=\u001b[39m fix(args, kw, sig)\n\u001b[0;32m--> 232\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcaller\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mextras\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/mari/lib/python3.9/site-packages/IPython/core/formatters.py:222\u001b[0m, in \u001b[0;36mcatch_format_error\u001b[0;34m(method, self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"show traceback on failed format call\"\"\"\u001b[39;00m\n\u001b[1;32m    221\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 222\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m:\n\u001b[1;32m    224\u001b[0m     \u001b[38;5;66;03m# don't warn on NotImplementedErrors\u001b[39;00m\n\u001b[1;32m    225\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_return(\u001b[38;5;28;01mNone\u001b[39;00m, args[\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/mari/lib/python3.9/site-packages/IPython/core/formatters.py:343\u001b[0m, in \u001b[0;36mBaseFormatter.__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    341\u001b[0m     method \u001b[38;5;241m=\u001b[39m get_real_method(obj, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_method)\n\u001b[1;32m    342\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m method \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 343\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    344\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/mari/lib/python3.9/site-packages/PIL/Image.py:717\u001b[0m, in \u001b[0;36mImage._repr_png_\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    712\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_repr_png_\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbytes\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    713\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"iPython display hook support for PNG format.\u001b[39;00m\n\u001b[1;32m    714\u001b[0m \n\u001b[1;32m    715\u001b[0m \u001b[38;5;124;03m    :returns: PNG version of the image as bytes\u001b[39;00m\n\u001b[1;32m    716\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 717\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_repr_image\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPNG\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompress_level\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/mari/lib/python3.9/site-packages/PIL/Image.py:707\u001b[0m, in \u001b[0;36mImage._repr_image\u001b[0;34m(self, image_format, **kwargs)\u001b[0m\n\u001b[1;32m    705\u001b[0m b \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mBytesIO()\n\u001b[1;32m    706\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 707\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_format\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    708\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m    709\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/mari/lib/python3.9/site-packages/PIL/Image.py:2588\u001b[0m, in \u001b[0;36mImage.save\u001b[0;34m(self, fp, format, **params)\u001b[0m\n\u001b[1;32m   2585\u001b[0m     fp \u001b[38;5;241m=\u001b[39m cast(IO[\u001b[38;5;28mbytes\u001b[39m], fp)\n\u001b[1;32m   2587\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 2588\u001b[0m     \u001b[43msave_handler\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2589\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   2590\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m open_fp:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/mari/lib/python3.9/site-packages/PIL/PngImagePlugin.py:1495\u001b[0m, in \u001b[0;36m_save\u001b[0;34m(im, fp, filename, chunk, save_all)\u001b[0m\n\u001b[1;32m   1491\u001b[0m     single_im \u001b[38;5;241m=\u001b[39m _write_multiple_frames(\n\u001b[1;32m   1492\u001b[0m         im, fp, chunk, mode, rawmode, default_image, append_images\n\u001b[1;32m   1493\u001b[0m     )\n\u001b[1;32m   1494\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m single_im:\n\u001b[0;32m-> 1495\u001b[0m     \u001b[43mImageFile\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_save\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1496\u001b[0m \u001b[43m        \u001b[49m\u001b[43msingle_im\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mIO\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mbytes\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_idat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1498\u001b[0m \u001b[43m        \u001b[49m\u001b[43m[\u001b[49m\u001b[43mImageFile\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Tile\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mzip\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msingle_im\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrawmode\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1499\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1501\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m info:\n\u001b[1;32m   1502\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m info_chunk \u001b[38;5;129;01min\u001b[39;00m info\u001b[38;5;241m.\u001b[39mchunks:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/mari/lib/python3.9/site-packages/PIL/ImageFile.py:648\u001b[0m, in \u001b[0;36m_save\u001b[0;34m(im, fp, tile, bufsize)\u001b[0m\n\u001b[1;32m    646\u001b[0m     _encode_tile(im, fp, tile, bufsize, fh)\n\u001b[1;32m    647\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mAttributeError\u001b[39;00m, io\u001b[38;5;241m.\u001b[39mUnsupportedOperation) \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m--> 648\u001b[0m     \u001b[43m_encode_tile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbufsize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    649\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(fp, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mflush\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    650\u001b[0m     fp\u001b[38;5;241m.\u001b[39mflush()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/mari/lib/python3.9/site-packages/PIL/ImageFile.py:674\u001b[0m, in \u001b[0;36m_encode_tile\u001b[0;34m(im, fp, tile, bufsize, fh, exc)\u001b[0m\n\u001b[1;32m    671\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m exc:\n\u001b[1;32m    672\u001b[0m     \u001b[38;5;66;03m# compress to Python file-compatible object\u001b[39;00m\n\u001b[1;32m    673\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 674\u001b[0m         errcode, data \u001b[38;5;241m=\u001b[39m \u001b[43mencoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbufsize\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m:]\n\u001b[1;32m    675\u001b[0m         fp\u001b[38;5;241m.\u001b[39mwrite(data)\n\u001b[1;32m    676\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m errcode:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Here we open the default camera (index 0).\n",
    "# On macOS, CAP_AVFOUNDATION tends to be the most reliable backend for OpenCV.\n",
    "cap = cv2.VideoCapture(0, cv2.CAP_AVFOUNDATION)\n",
    "print(\"opened:\", cap.isOpened())\n",
    "\n",
    "try:\n",
    "    # Here we do one warm-up read.\n",
    "    # Some webcams return an empty/low-quality first frame while exposure/focus settles.\n",
    "    ok, frame = cap.read()\n",
    "    print(\"got first frame:\", ok)\n",
    "\n",
    "    while True:\n",
    "        ok, frame = cap.read()\n",
    "        if not ok:\n",
    "            # Instead of breaking (which kills the loop),\n",
    "            # Just skip this iteration and try again next tick\n",
    "            # Sometimes mac returns a bad frame when focus changes\n",
    "            time.sleep(0.01)\n",
    "            continue\n",
    "\n",
    "        # Mirror like a selfie. Purely stylistic choice.\n",
    "        frame = cv2.flip(frame, 1)\n",
    "        \n",
    "        # Here we convert to grayscale because dlib's detector works on single-channel images.\n",
    "        gray  = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # Here we detect faces. The second argument is the upsample count:\n",
    "        # 0 is faster, higher values can detect smaller faces but cost more CPU.\n",
    "        rects = detector(gray, 0)\n",
    "\n",
    "        for rect in rects:\n",
    "            # Here we extract the bounding box and draw it for visual debugging.\n",
    "            x1,y1,x2,y2 = rect.left(), rect.top(), rect.right(), rect.bottom()\n",
    "            cv2.rectangle(frame, (x1,y1), (x2,y2), (0,255,0), 2)\n",
    "\n",
    "            # 68 landmarks\n",
    "            pts = shape_to_np(predictor(gray, rect))\n",
    "\n",
    "            # Normalize by distance between outer eye corners\n",
    "            span = dist(pts[IDX[\"eye_L_outer\"]], pts[IDX[\"eye_R_outer\"]]) + 1e-6\n",
    "\n",
    "            # Features\n",
    "\n",
    "            # Here we compute mouth metrics:\n",
    "            # - mouth_width: smile/grin tends to widen the mouth\n",
    "            # - mouth_open: surprise/talking tends to increase inner lip distance\n",
    "            mouth_width = dist(pts[IDX[\"mouth_left\"]], pts[IDX[\"mouth_right\"]]) / span\n",
    "            mouth_open  = dist(pts[IDX[\"mouth_in_top\"]], pts[IDX[\"mouth_in_bot\"]]) / span\n",
    "\n",
    "            # Here we compute eye openness using EAR (Eye Aspect Ratio).\n",
    "            # EAR drops when the eye closes and rises when the eye opens wide.\n",
    "            eyeL_pts, eyeR_pts = pts[36:42], pts[42:48]\n",
    "            earL, earR = eye_ear(eyeL_pts), eye_ear(eyeR_pts)\n",
    "            eye_open = 0.5*(earL + earR)\n",
    "\n",
    "            # Here we compute approximate centers for eyes and brows.\n",
    "            # We use these to estimate \"brow gap\" (raised brows vs lowered brows).\n",
    "            eyeL_c, eyeR_c   = center(pts[IDX[\"eye_L\"]]), center(pts[IDX[\"eye_R\"]])\n",
    "            browL_c, browR_c = center(pts[IDX[\"brow_L\"]]), center(pts[IDX[\"brow_R\"]])\n",
    "\n",
    "            # Here we estimate brow raise/lower by measuring vertical distance from brow to eye.\n",
    "            # Higher values typically mean \"raised brows\" (surprise), lower values mean \"compressed brows\".\n",
    "            brow_gap = 0.5 * (\n",
    "                abs(browL_c[1] - eyeL_c[1]) / span +\n",
    "                abs(browR_c[1] - eyeR_c[1]) / span\n",
    "            )\n",
    "\n",
    "            # Here we measure \"brow compression\" for an anger-ish signal.\n",
    "            # We compare inner brow gap vs outer brow gap:\n",
    "            # - inner brow pulling down tends to shrink the inner gap more than the outer gap\n",
    "            inner_gap = (abs(pts[21,1]-eyeL_c[1]) + abs(pts[22,1]-eyeR_c[1]))/(2*span)\n",
    "            outer_gap = (abs(pts[17,1]-eyeL_c[1]) + abs(pts[26,1]-eyeR_c[1]))/(2*span)\n",
    "            delta_brow = outer_gap - inner_gap\n",
    "\n",
    "            # Mouth curvature: <0 smile, >0 frown\n",
    "            center_y  = 0.5*(pts[51,1] + pts[57,1])\n",
    "            corners_y = 0.5*(pts[48,1] + pts[54,1])\n",
    "            curve     = (corners_y - center_y) / span\n",
    "\n",
    "            # classify\n",
    "            # Here we classify the current frame using the heuristic rules.\n",
    "            raw = label_from_feats(\n",
    "                mouth_width, mouth_open, brow_gap,\n",
    "                curve, eye_open, delta_brow\n",
    "            )\n",
    "\n",
    "            # Here we smooth the output across multiple frames.\n",
    "            # We store the last N labels and take the most common one to reduce flicker.\n",
    "            smooth_labels.append(raw)\n",
    "            lbl = Counter(smooth_labels).most_common(1)[0][0]\n",
    "\n",
    "            # Speak if needed (async)\n",
    "            maybe_say(lbl)\n",
    "\n",
    "            # Here we draw the final label above the face box.\n",
    "            cv2.putText(frame, lbl, (x1, y1-10),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.9, COL[lbl], 2)\n",
    "            \n",
    "            # Here we display the live feature values so threshold tuning is easier.\n",
    "            cv2.putText(\n",
    "                frame,\n",
    "                f\"w:{mouth_width:.2f} o:{mouth_open:.2f} b:{brow_gap:.2f} ear:{eye_open:.2f} Δb:{delta_brow:.2f}\",\n",
    "                (10,50),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                0.7,\n",
    "                (255,255,255),\n",
    "                2\n",
    "            )\n",
    "            # Here we draw all 68 landmarks as small dots (visual sanity check).\n",
    "            for (px,py) in pts.astype(int):\n",
    "                cv2.circle(frame, (px,py), 2, (0,255,255), -1)\n",
    "\n",
    "        # Display face count\n",
    "        cv2.putText(frame, f\"faces: {len(rects)}\", (10,25),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255,255,255), 2)\n",
    "\n",
    "        # Show in notebook\n",
    "        rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        clear_output(wait=True)\n",
    "        display(Image.fromarray(rgb))\n",
    "\n",
    "        # Tiny sleep so notebook doesn't get spammed\n",
    "        time.sleep(0.005)\n",
    "\n",
    "finally:\n",
    "    # Here we always release the camera, even if the loop crashes or the notebook is interrupted.\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    print(\"camera released\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be2990d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mari",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
